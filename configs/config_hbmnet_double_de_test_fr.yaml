exp_name: hbmnet_w_double_attention_de_test_fr

data:
  dataset: hbmdataset
  train:
    data_path: data/db_hbm/prepared_data_w_dum_data_de_topics.p  # prepared_data_v2.p
    txt_feat_path: features/twitterxlmrobertabase_text_embed_w_de_topics.p
    img_feat_path: features/imagenet_img_embed_w_de_topics.p
  test:
    data_path: data/db_hbm/prepared_data_w_dum_data_en_topics.p  # prepared_data_v2.p
    txt_feat_path: features/twitterxlmrobertabase_text_embed_w_en_topics.p
    img_feat_path: features/imagenet_img_embed_w_en_topics.p
  normalize: False
  val_percent: 0.15
  test_percent: 0
  batch_size: 128
  num_workers: 8
  limit_n_replies: 100
  limit_n_quotes: 100
  use_img_feat: True
  use_like_feat: False

# models: hbmnet | hbmnet_t | hbmnet_tv | hbmnet_w_rep_imgt | hbmnet_att | hbmnet_att_like | hbmnet_self_att | hbmnet_double_att
model: hbmnet_double_att
model_path: exps/089_hbmnet_w_double_attention_de/0/checkpoints/hbmnet_imagenet_feats_twitterxlmrobertabase_sumavg_final.pt
fold: 0

training:
  multilabel: False
  epochs: 1000
  patience: 20
  lr_patience: 5
  init_lr: 0.0001
  output_path: exps
  save_every: 50
  cv:
    stratified: True
    k_folds: 3